# 强度学习心得


## 未来收益之和Reward概念的理解

强化学习讲究要让我们的每一步action收益(reward)最大，这里的reward是针对于未来说的。我现在还不知道未来的情况是啥，但是我需要计算从我现在这一步St到我结束的全部受益。


## 函数更新策略
> MC算法和TD算法

[参考博客](https://zhuanlan.zhihu.com/p/94464246)


## 名词解释

**advantage function**：Usually denoted as A(s,a), the Advantage function is a measure of how much is a certain action a good or bad decision given a certain state

## 常见的算法

![](../imgs/DRLAlogrithm.png)


## DQN network

Bellman方程


### 为什么DQN需要两个网络
[参考链接](https://ai.stackexchange.com/questions/22504/why-do-we-need-target-network-in-deep-q-learning)

之前一直有一个疑问就是为什么不能直接一个网络来完成TD方法的更新？

DQN的核心公式是：(rt+maxaQ(st+1,a;θ−)−Q(st,at;θ))2 ，其中θ- 是来自Target Network里面的，这样是为了让Action网络去暂时的去接近这个Target网络，来保证自己可以在迭代C次之间逐步找到一个最优解。

> 但是如果我就只用一个网络去做呢？？

也就是说都用θ来更新，这样会使得网络变得不稳定，不知道网络去哪里优化，因为它的最优解目标一直在变。

上面的那个参考博客说的很好，就像一个狗在尝试转圈咬到自己的尾巴一样。


